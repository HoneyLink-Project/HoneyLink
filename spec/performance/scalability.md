# docs/performance/scalability.md

**バッジ:** `🚫 実装コード非出力` `🚫 C/C++依存禁止`

> HoneyLink™ プロトコルの性能・スケーラビリティ指針を定義します。Rust やマネージドサービスを優先し、C/C++ 依存や実装コードは含めません。

## 目次
- [性能ビジョンと前提](#性能ビジョンと前提)
- [ターゲットワークロードとキャパシティ](#ターゲットワークロードとキャパシティ)
- [スケール戦略](#スケール戦略)
- [キャパシティプランニング手順](#キャパシティプランニング手順)
- [観測性と自動スケール制御](#観測性と自動スケール制御)
- [レイテンシバジェットとQoS](#レイテンシバジェットとqos)
- [レジリエンスと優雅な劣化](#レジリエンスと優雅な劣化)
- [依存関係と制約](#依存関係と制約)
- [受け入れ基準 (DoD)](#受け入れ基準-dod)

## 性能ビジョンと前提
- 目標: エッジ〜クラウド 1 億デバイス規模で平均 RTT 45ms 以下、99.9 パーセンタイル 120ms 以下。
- ハードリアルタイムではなくソフトリアルタイムを想定。Critical Control Path は 30ms 以内を KPI とする。
- すべての高負荷処理は非同期 Rust サービスまたはマネージドストリーム基盤を利用し、C/C++ 実装は採用しない。
- 詳細な要求値は [docs/requirements.md](../requirements.md) の非機能要件を参照。

## ターゲットワークロードとキャパシティ
| トラフィック種別 | 平均 TPS | ピーク TPS (P95) | バースト幅 | 備考 |
|------------------|---------|------------------|-----------|------|
| Telemetry uplink | 18k | 45k | 3 分 | 圧縮後 1.2KB/メッセージ |
| Command downlink | 4k | 12k | 30 秒 | 優先度キューで制御 |
| Pairing handshake | 2.5k | 6k | 5 分 | 新規導入キャンペーン時 |
| Firmware OTA | 0.8k | 2k | 60 分 | 差分配信、夜間バッチ |

- キャパシティ測定は 5 分粒度で記録し、90 日ローリングで動的閾値を再計算。
- Region 当たりのテナント数は 2 万を上限とし、越える場合はリージョン分割を推奨。

## スケール戦略
### コントロールプレーン
- **水平分割:** テナントシャーディング + Consistent Hashing。各シャードは独立した Rust サービスを運用。
- **ステート管理:** マネージドキー・バリューストア (FoundationDB, DynamoDB 等) を使用。
- **バックプレッシャ:** gRPC backoff とリトライ制御で負荷平準化。

### データプレーン
- **マルチストリーム QoS:** `critical`, `standard`, `bulk` の 3 階層。critical は予約帯域 25%、標準 60%、バルク 15%。
- **プロトコル圧縮:** Header Compression + Binary TLV フォーマット。暗号オーバーヘッドを考慮した MTU チューニング。
- **エッジキャッシュ:** ローカル処理優先でクラウド転送を 40% 削減。

### ストレージ
- タイムシリーズデータはカラムナー DB (マネージド) で 30 日保持。冷データはオブジェクトストレージへ自動移行。
- 書き込みピークは 15MB/s/シャードを設計上限。超過時はバッファリングキューにオフロード。

## キャパシティプランニング手順
1. [docs/roadmap.md](../roadmap.md) のマイルストーンに基づき四半期ごとに需要予測を更新。
2. 現行メトリクス (CPU, メモリ, ネットワーク, ストレージ IOPS) を集約し、P95 使用率が 60% 超のリソースを特定。
3. 分析結果からシャード追加 or リージョン分割を決定し、[docs/deployment/infrastructure.md](../deployment/infrastructure.md) に変更を反映。
4. 変更は Infrastructure as Code (Terraform/Bicep) でローリング適用。C/C++ 製ツールチェーンは禁止。
5. 適用後 2 週間の観測データで効果検証、KPI 達成状況を [docs/testing/metrics.md](../testing/metrics.md) に更新。

## 観測性と自動スケール制御
- OpenTelemetry Collector → マネージドモニタリング (Azure Monitor, CloudWatch 等) へストリーム。
- HPA/KEDA ルール: `CPU 60%`, `メモリ 70%`, `TPS`, `Queue depth`。複合ルールで誤トリガーを抑制。
- 異常検知: Holt-Winters + ARIMA のハイブリッド予測。Rust 実装ライブラリを採用し、C/C++ 依存は排除。
- キャパシティ警告は Slack / PagerDuty 連携。5 分以内のエスカレーションを SLA 化。

## レイテンシバジェットとQoS
- レイテンシバジェットを `ネットワーク 40%`, `暗号処理 25%`, `ビジネスロジック 20%`, `ストレージ 15%` に配分。
- ビジネスロジック超過時は機能フラグでコストの高い機能を一時停止。
- QoS は Weighted Fair Queuing。`critical` キューは Starvation 回避のため `standard` から 10% を最低保証として借用可能。

## レジリエンスと優雅な劣化
- 3 AZ 構成 + クロスリージョンレプリケーション。単一リージョン喪失時も 60% 処理能力を維持。
- グレースフルデグレード: OTA を停止、Bulk チャネルを 24 時間まで遅延許容。
- マルチクラウド戦略: 控えリージョンをホットスタンバイとして維持し、DR は 30 分以内。
- カオス検証は月次で実施。結果は [docs/testing/integration-tests.md](../testing/integration-tests.md) にフィードバック。

## 依存関係と制約
- ネットワーク: IPv6 優先、QUIC + TLS 1.3 のみサポート。
- 暗号: [docs/security/encryption.md](../security/encryption.md) の鍵ローテポリシーを遵守。
- CI/CD: スケール設定は GitOps ワークフロー経由で適用。手動変更は禁止。
- 法令: エッジデータは地域データレジデンシ要件を満たすクラウドに配置。

## 受け入れ基準 (DoD)
- 性能ビジョン・SLO が明文化され、要求文書・ロードマップと整合している。
- スケール戦略がコントロールプレーン／データプレーン／ストレージの観点で具体化されている。
- キャパシティプランニングと自動スケール制御の手順が観測性メカニズムと連動している。
- レイテンシバジェットと優雅な劣化手順が記述されている。
- C/C++ 依存がないこと、関連する他ドキュメントへのリンクが提供されている。
